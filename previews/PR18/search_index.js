var documenterSearchIndex = {"docs":
[{"location":"merge/#Merging-TMLE-and-SVP-outputs","page":"Merging TMLE and SVP outputs","title":"Merging TMLE and SVP outputs","text":"","category":"section"},{"location":"merge/","page":"Merging TMLE and SVP outputs","title":"Merging TMLE and SVP outputs","text":"If multiple scripts/tmle.jl and potentially scripts/sieve_variance.jl have been run, you may want to combine the generated CSV outputs in a single result file. This is the purpose of this command line interface.","category":"page"},{"location":"merge/#Usage","page":"Merging TMLE and SVP outputs","title":"Usage","text":"","category":"section"},{"location":"merge/","page":"Merging TMLE and SVP outputs","title":"Merging TMLE and SVP outputs","text":"You can merge summary CSV files by running:","category":"page"},{"location":"merge/","page":"Merging TMLE and SVP outputs","title":"Merging TMLE and SVP outputs","text":"julia scripts/merge_summaries.jl TMLE_PREFIX OUT --sieve-prefix=SIEVE_PREFIX","category":"page"},{"location":"merge/","page":"Merging TMLE and SVP outputs","title":"Merging TMLE and SVP outputs","text":"where:","category":"page"},{"location":"merge/","page":"Merging TMLE and SVP outputs","title":"Merging TMLE and SVP outputs","text":"TMLE_PREFIX: is a prefix to all output CSV files generated by the scripts/tmle.jl script.\nOUT: is a path to the output file that will be generated.\n--sieve-prefix: is an optional prefix to the CSV output of the scripts/sieve_variance.jl script.","category":"page"},{"location":"environment/#The-Run-Environment","page":"The Run Environment","title":"The Run Environment","text":"","category":"section"},{"location":"environment/#General-usage","page":"The Run Environment","title":"General usage","text":"","category":"section"},{"location":"environment/","page":"The Run Environment","title":"The Run Environment","text":"At this point in time, the package depends on several R dependencies which makes it difficult to package as a single Julia executable. We thus rely on a docker container for the execution of the various command line interfaces. ","category":"page"},{"location":"environment/","page":"The Run Environment","title":"The Run Environment","text":"This container is available for download from the docker registry. \nIn this container, the project is stored in /TargetedEstimation.jl, as such, any script can be run using the following template command: julia --startup-file=no --project=/TargetedEstimation.jl /TargetedEstimation.jl/scripts/SCRIPT_NAME.jl. Dont forget to mount the output directory in order to retrieve the output data.","category":"page"},{"location":"environment/#Alternatives","page":"The Run Environment","title":"Alternatives","text":"","category":"section"},{"location":"environment/","page":"The Run Environment","title":"The Run Environment","text":"Here are a couple alternatives to using the docker container:","category":"page"},{"location":"environment/","page":"The Run Environment","title":"The Run Environment","text":"If you are not using the HAL algorithm, you can simply clone this repository and instantiate the project in order to use the scripts or any other functionality.\nIf you are using the HAL algorithm you can use the docker/Dockerfile as a guide for your local installation","category":"page"},{"location":"models/#Models","page":"Models","title":"Models","text":"","category":"section"},{"location":"models/","page":"Models","title":"Models","text":"CurrentModule = TargetedEstimation","category":"page"},{"location":"models/","page":"Models","title":"Models","text":"Because TMLE.jl is based on top of MLJ, we can support any model respecting the MLJ interface. At the moment, we readily support all models from the following packages:","category":"page"},{"location":"models/","page":"Models","title":"Models","text":"MLJLinearModels: Generalized Linear Models in Julia.\nXGBoost.jl: Julia wrapper of the famous XGBoost package.\nEvoTrees.jl: A pure Julia implementation of histogram based gradient boosting trees (subset of XGBoost)\nGLMNet: A Julia wrapper of the glmnet package. See GLMNet.\nMLJModels: General utilities such as the OneHotEncoder or InteractionTransformer.\nHighlyAdaptiveLasso: A Julia wrapper of the HAL algorithm, experimental.","category":"page"},{"location":"models/","page":"Models","title":"Models","text":"Further support for more packages can be added on request, please fill an issue.","category":"page"},{"location":"models/","page":"Models","title":"Models","text":"Also, because the Estimator File is a pure Julia file, it is possible to use it in order to install additional package that can be used to define additional models.","category":"page"},{"location":"models/","page":"Models","title":"Models","text":"Finally, we also provide some additional models described in Additional models provided by TargetedEstimation.jl.","category":"page"},{"location":"models/#Additional-models-provided-by-TargetedEstimation.jl","page":"Models","title":"Additional models provided by TargetedEstimation.jl","text":"","category":"section"},{"location":"models/#GLMNet","page":"Models","title":"GLMNet","text":"","category":"section"},{"location":"models/","page":"Models","title":"Models","text":"This is a simple wrapper around the glmnetcv function from the GLMNet.jl package. The only difference is that the resampling is made based on MLJ resampling strategies.","category":"page"},{"location":"models/","page":"Models","title":"Models","text":"GLMNetRegressor(;resampling=CV(), params...)","category":"page"},{"location":"models/#TargetedEstimation.GLMNetRegressor-Tuple{}","page":"Models","title":"TargetedEstimation.GLMNetRegressor","text":"GLMNetRegressor(;resampling=CV(), params...)\n\nA GLMNet regressor for continuous outcomes based on the glmnetcv function from the GLMNet.jl  package.\n\nArguments:\n\nresampling: A MLJ ResamplingStrategy, see MLJ resampling strategies\nparams: Additional parameters to the glmnetcv function\n\nExamples:\n\nA glmnet with alpha=0.\n\n\nmodel = GLMNetRegressor(resampling=CV(nfolds=3), alpha=0)\nmach = machine(model, X, y)\nfit!(mach, verbosity=0)\n\n\n\n\n\n","category":"method"},{"location":"models/","page":"Models","title":"Models","text":"GLMNetClassifier(;resampling=StratifiedCV(), params...)","category":"page"},{"location":"models/#TargetedEstimation.GLMNetClassifier-Tuple{}","page":"Models","title":"TargetedEstimation.GLMNetClassifier","text":"GLMNetClassifier(;resampling=StratifiedCV(), params...)\n\nA GLMNet classifier for binary/multinomial outcomes based on the glmnetcv function from the GLMNet.jl  package.\n\nArguments:\n\nresampling: A MLJ ResamplingStrategy, see MLJ resampling strategies\nparams: Additional parameters to the glmnetcv function\n\nExamples:\n\nA glmnet with alpha=0.\n\n\nmodel = GLMNetClassifier(resampling=StratifiedCV(nfolds=3), alpha=0)\nmach = machine(model, X, y)\nfit!(mach, verbosity=0)\n\n\n\n\n\n","category":"method"},{"location":"models/#RestrictedInteractionTransformer","page":"Models","title":"RestrictedInteractionTransformer","text":"","category":"section"},{"location":"models/","page":"Models","title":"Models","text":"This transformer generates interaction terms based on a set of primary variables in order to limit the combinatorial explosion.","category":"page"},{"location":"models/","page":"Models","title":"Models","text":"RestrictedInteractionTransformer","category":"page"},{"location":"models/#TargetedEstimation.RestrictedInteractionTransformer","page":"Models","title":"TargetedEstimation.RestrictedInteractionTransformer","text":"RestrictedInteractionTransformer(;order=2, primary_variables=Symbol[], primary_variables_patterns=Regex[])\n\nDefinition\n\nThis transformer generates interaction terms based on a set of primary variables. All generated interaction terms  are composed of a set of primary variables and at most one remaining variable in the provided table. If (T‚ÇÅ, T‚ÇÇ) are defining the set of primary variables and (W‚ÇÅ, W‚ÇÇ) are reamining variables in the table, the generated interaction terms at order 2  will be:\n\nT‚ÇÅxT‚ÇÇ\nT‚ÇÅxW‚ÇÇ\nW‚ÇÅxT‚ÇÇ\n\nbut W‚ÇÅxW‚ÇÇ will not be generated because it would contain 2 remaining variables.\n\nArguments:\n\norder: All interaction features up to the given order will be computed\nprimary_variables: A set of column names to generate the interactions\nprimaryvariablespatterns: A set of regular expression that can additionally \n\nbe used to identify primary_variables.\n\n\n\n\n\n","category":"type"},{"location":"models/#BiAllelicSNPEncoder","page":"Models","title":"BiAllelicSNPEncoder","text":"","category":"section"},{"location":"models/","page":"Models","title":"Models","text":"This transformer, mostly useful for genetic studies, converts bi-allelic single nucleotide polyphormism columns, encoded as Strings to a count of one of the two alleles.","category":"page"},{"location":"models/","page":"Models","title":"Models","text":"BiAllelicSNPEncoder","category":"page"},{"location":"models/#TargetedEstimation.BiAllelicSNPEncoder","page":"Models","title":"TargetedEstimation.BiAllelicSNPEncoder","text":"BiAllelicSNPEncoder(patterns=Symbol[])\n\nEncodes bi-allelic SNP columns, identified by the provided patterns Regex,  as a count of a reference allele determined dynamically (not necessarily the minor allele).\n\n\n\n\n\n","category":"type"},{"location":"sieve_variance/#Sieve-Variance-Plateau-Estimation","page":"Sieve Variance Plateau Estimation","title":"Sieve Variance Plateau Estimation","text":"","category":"section"},{"location":"sieve_variance/","page":"Sieve Variance Plateau Estimation","title":"Sieve Variance Plateau Estimation","text":"If the i.i.d. (independent and identically distributed) hypothesis is not satisfied, most of the traditional statistical inference theory falls apart. This is typically possible in population genetics where a study may contain related individuals. Here we leverage a non-parametric method called Sieve Variance Plateau (SVP) estimation. The hypothesis is that the dependence between individuals is sufficiently small, so that our targeted estimator will still be asymptotically unbiased, but its variance will be under estimated. In brief, the SVP estimator computes a variance estimate for a range of thresholds ùúè, by considering individuals to be independent if their distance exceeds ùúè. As the distance threshold ùúè increases, fewer individuals are assumed to be independent. The maximum of this curve is the most conservative estimate of the variance of the target parameter estimator and constitutes our SVP corrected variance estimator.","category":"page"},{"location":"sieve_variance/#Usage","page":"Sieve Variance Plateau Estimation","title":"Usage","text":"","category":"section"},{"location":"sieve_variance/","page":"Sieve Variance Plateau Estimation","title":"Sieve Variance Plateau Estimation","text":"At the moment, this script is restricted to the analysis of population genetics datasets mostly in the context of TarGene. It can be run with the following command:","category":"page"},{"location":"sieve_variance/","page":"Sieve Variance Plateau Estimation","title":"Sieve Variance Plateau Estimation","text":"julia scripts/sieve_variance.jl PREFIX GRM_PREFIX OUT_PREFIX\n        --nb-estimators=100\n        --max-tau=1.0\n        --verbosity=1","category":"page"},{"location":"sieve_variance/","page":"Sieve Variance Plateau Estimation","title":"Sieve Variance Plateau Estimation","text":"where:","category":"page"},{"location":"sieve_variance/","page":"Sieve Variance Plateau Estimation","title":"Sieve Variance Plateau Estimation","text":"PREFIX: A prefix to HDF5 files generated by scripts/tmle.jl (potentially multiple).\nGRM_PREFIX: A prefix to the aggregated Genetic Relationship Matrix.\nOUT_PREFIX: Output prefix to save SVP curves and final variance estimates.\n--nb-estimators: The number of points per SVP curve.\n--max-tau: Maximum distance between individuals to consider.\n--verbosity: Verbosity level.","category":"page"},{"location":"resampling/#Resampling-Strategies","page":"Resampling Strategies","title":"Resampling Strategies","text":"","category":"section"},{"location":"resampling/","page":"Resampling Strategies","title":"Resampling Strategies","text":"CurrentModule = TargetedEstimation","category":"page"},{"location":"resampling/","page":"Resampling Strategies","title":"Resampling Strategies","text":"We also provide additional resampling strategies compliant with the MLJ.ResamplingStrategy interface.","category":"page"},{"location":"resampling/#AdaptiveResampling","page":"Resampling Strategies","title":"AdaptiveResampling","text":"","category":"section"},{"location":"resampling/","page":"Resampling Strategies","title":"Resampling Strategies","text":"The AdaptiveResampling strategies will determine the number of cross-validation folds adaptively based on the available data. This is inspired from the this paper on practical considerations for super learning.","category":"page"},{"location":"resampling/","page":"Resampling Strategies","title":"Resampling Strategies","text":"The AdaptiveCV will determine the number of folds adaptively and perform a classic cross-validation split:","category":"page"},{"location":"resampling/","page":"Resampling Strategies","title":"Resampling Strategies","text":"AdaptiveCV","category":"page"},{"location":"resampling/#TargetedEstimation.AdaptiveCV","page":"Resampling Strategies","title":"TargetedEstimation.AdaptiveCV","text":"AdaptiveCV(;shuffle=nothing, rng=nothing)\n\nA CV (see MLJBase.CV) resampling strategy where the number of folds is determined  data adaptively based on the rule of thum described here.\n\n\n\n\n\n","category":"type"},{"location":"resampling/","page":"Resampling Strategies","title":"Resampling Strategies","text":"The AdaptiveStratifiedCV will determine the number of folds adaptively and perform a stratified cross-validation split:","category":"page"},{"location":"resampling/","page":"Resampling Strategies","title":"Resampling Strategies","text":"AdaptiveStratifiedCV","category":"page"},{"location":"resampling/#TargetedEstimation.AdaptiveStratifiedCV","page":"Resampling Strategies","title":"TargetedEstimation.AdaptiveStratifiedCV","text":"AdaptiveStratifiedCV(;shuffle=nothing, rng=nothing)\n\nA StratifiedCV (see MLJBase.StratifiedCV) resampling strategy where the number of folds is determined  data adaptively based on the rule of thum described here.\n\n\n\n\n\n","category":"type"},{"location":"resampling/#JointStratifiedCV","page":"Resampling Strategies","title":"JointStratifiedCV","text":"","category":"section"},{"location":"resampling/","page":"Resampling Strategies","title":"Resampling Strategies","text":"Sometimes, the treatment variables (or some other features) are imbalanced and naively performing cross-validation or stratified cross-validation could result in the violation of the positivity hypothesis. To overcome this difficulty, the following JointStratifiedCV, performs a stratified cross-validation based on both features variables and the outcome variable.","category":"page"},{"location":"resampling/","page":"Resampling Strategies","title":"Resampling Strategies","text":"JointStratifiedCV","category":"page"},{"location":"resampling/#TargetedEstimation.JointStratifiedCV","page":"Resampling Strategies","title":"TargetedEstimation.JointStratifiedCV","text":"JointStratifiedCV(;patterns=nothing, resampling=StratifiedCV())\n\nApplies a stratified cross-validation strategy based on a variable constructed from X and y.  A composite variable is built from: \n\nx variables from X matching any of patterns and satisfying autotype(x) <: Union{Missing, Finite}. \n\nIf no pattern is provided, then only the second condition is considered.\n\ny if autotype(y) <: Union{Missing, Finite}\n\nThe resampling needs to be a stratification compliant resampling strategy, at the moment  one of StratifiedCV or AdaptiveStratifiedCV\n\n\n\n\n\n","category":"type"},{"location":"#TargetedEstimation.jl","page":"Home","title":"TargetedEstimation.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The goal of this package, eventually, is to provide a standalone executable to run Targeted Minimum Loss-based Estimation (TMLE) on large scale tabular datasets. It is based on the companion TMLE.jl package.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The various command line interfaces are described in the following sections:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Targeted Minimum Loss Based Estimation: The main command line interface provided in this project to run TMLE.\nSieve Variance Plateau Estimation: Variance correction for non i.i.d. data.","category":"page"},{"location":"","page":"Home","title":"Home","text":"We also provide extensions to the MLJ universe that are particularly useful in statistical genetics (but not restricted to it):","category":"page"},{"location":"","page":"Home","title":"Home","text":"Additional Models\nAdditional Resampling Strategies","category":"page"},{"location":"tmle_estimation/#Targeted-Minimum-Loss-Based-Estimation","page":"Targeted Minimum Loss Based Estimation","title":"Targeted Minimum Loss Based Estimation","text":"","category":"section"},{"location":"tmle_estimation/","page":"Targeted Minimum Loss Based Estimation","title":"Targeted Minimum Loss Based Estimation","text":"This is the main script in this package, it provides a command line interface for the estimation of statistical parameters using targeted Learning. ","category":"page"},{"location":"tmle_estimation/#Usage","page":"Targeted Minimum Loss Based Estimation","title":"Usage","text":"","category":"section"},{"location":"tmle_estimation/","page":"Targeted Minimum Loss Based Estimation","title":"Targeted Minimum Loss Based Estimation","text":"Provided you have the package and all dependencies installed or in the provided docker container, you can run TMLE via the following command:","category":"page"},{"location":"tmle_estimation/","page":"Targeted Minimum Loss Based Estimation","title":"Targeted Minimum Loss Based Estimation","text":"julia scripts/tmle.jl DATAFILE PARAMFILE OUTFILE\n        --estimator-file=docs/estimators/glmnet.jl\n        --hdf5-out=output.hdf5\n        --pval-threshold=0.05\n        --chunksize=100\n        --verbosity=1","category":"page"},{"location":"tmle_estimation/","page":"Targeted Minimum Loss Based Estimation","title":"Targeted Minimum Loss Based Estimation","text":"where:","category":"page"},{"location":"tmle_estimation/","page":"Targeted Minimum Loss Based Estimation","title":"Targeted Minimum Loss Based Estimation","text":"DATAFILE: A CSV (.csv) or Arrow (.arrow) file containing the tabular data. The format will be deduced from the extension.\nPARAMFILE: A serialized YAML or bin file containing the estimands to be estimated. The YAML file can be written by hand or programmatically using the TMLE.parameterstoyaml function.\nOUTFILE: The output .csv file\n--estimator-file: A Julia file describing the TMLE specifications (see Estimator File).\n--hdf5-out: if provided, a path to a file to save the influence curves.\n--pval-threshold: Only \"significant\" (< this threshold) estimates will actually have their influence curves stored in the previous file.\n--chunksize: To manage memory, the results are appended to the output files in batches the size of which can be controlled via this option.\n--verbosity: The verbosity level.","category":"page"},{"location":"tmle_estimation/#Estimator-File","page":"Targeted Minimum Loss Based Estimation","title":"Estimator File","text":"","category":"section"},{"location":"tmle_estimation/","page":"Targeted Minimum Loss Based Estimation","title":"Targeted Minimum Loss Based Estimation","text":"TMLE is an adaptive procedure that depends on the specification of learning algorithms for the estimation of the nuisance parameters (see TMLE.jl for a description of the assumed setting). In our case, there are two nuisance parameters for which we need to specify learning algorithms:","category":"page"},{"location":"tmle_estimation/","page":"Targeted Minimum Loss Based Estimation","title":"Targeted Minimum Loss Based Estimation","text":"E[Y|T, W, C]: The mean outcome given the treatment, confounders and extra covariates. It is commonly denoted by Q in the Targeted Learning litterature.\np(T|W): The propensity score. It is commonly denoted by G in the Targeted Learning litterature.","category":"page"},{"location":"tmle_estimation/#Description-of-the-file","page":"Targeted Minimum Loss Based Estimation","title":"Description of the file","text":"","category":"section"},{"location":"tmle_estimation/","page":"Targeted Minimum Loss Based Estimation","title":"Targeted Minimum Loss Based Estimation","text":"In order to provide maximum flexibility as to the choice of learning algorithms, the estimator file is a plain Julia file. This file is optional and omitting it defaults to using generalized linear models. If provided, it must define a NamedTuple called tmle_spec containing any of the following fields as follows (default configuration):","category":"page"},{"location":"tmle_estimation/","page":"Targeted Minimum Loss Based Estimation","title":"Targeted Minimum Loss Based Estimation","text":"\ntmle_spec = (\n  Q_continuous = LinearRegressor(),\n  Q_binary     = LogisticClassifier(lambda=0.),\n  G            = LogisticClassifier(lambda=0.),\n  threshold    = 1e-8,\n  cache        = false,\n  weighted_fluctuation = false\n)","category":"page"},{"location":"tmle_estimation/","page":"Targeted Minimum Loss Based Estimation","title":"Targeted Minimum Loss Based Estimation","text":"where:","category":"page"},{"location":"tmle_estimation/","page":"Targeted Minimum Loss Based Estimation","title":"Targeted Minimum Loss Based Estimation","text":"Q_continuous: is a MLJ model used for the estimation of E[Y|T, W, C] when the outcome Y is continuous.\nQ_binary: is a MLJ model used for the estimation of E[Y|T, W, C] when the outcome Y is binary.\nG: is a MLJ model used for the estimation of p(T|W).\nthreshold: is the minimum value the propensity score G is allowed to take.\ncache: controls caching of data by MLJ machines. Setting it to true may result in faster runtime but higher memory usage.\nweighted_fluctuation: controls whether the fluctuation for Q is a weighted glm or not.","category":"page"},{"location":"tmle_estimation/","page":"Targeted Minimum Loss Based Estimation","title":"Targeted Minimum Loss Based Estimation","text":"Typically, Q_continuous, Q_binary and G will be adjusted and other fields can be left unspecified.","category":"page"},{"location":"tmle_estimation/#Ready-to-use-estimator-files","page":"Targeted Minimum Loss Based Estimation","title":"Ready to use estimator files","text":"","category":"section"},{"location":"tmle_estimation/","page":"Targeted Minimum Loss Based Estimation","title":"Targeted Minimum Loss Based Estimation","text":"We recognize not everyone will be familiar with Julia. We thus provide a set of ready to use estimator files that can be simplified or extended as needed:","category":"page"},{"location":"tmle_estimation/","page":"Targeted Minimum Loss Based Estimation","title":"Targeted Minimum Loss Based Estimation","text":"Super Learning: with and without interaction terms in the GLM models for Q.\nSuper Learning for G and GLMNet for Q: here.\nSuper Learning for G and GLM for Q: here.\nGLMNet: with and without interaction terms in the GLM models for Q.\nGLM: with and without interaction terms in the GLM models for Q.\nXGBoost: with tuning.","category":"page"},{"location":"tmle_estimation/#Runtime","page":"Targeted Minimum Loss Based Estimation","title":"Runtime","text":"","category":"section"},{"location":"tmle_estimation/","page":"Targeted Minimum Loss Based Estimation","title":"Targeted Minimum Loss Based Estimation","text":"Targeted Learning can quickly become computationally intensive compared to traditional parametric inference. Here, we illustrate typical runtimes using examples from population genetics. This is because population genetics is currently the main use case for this package, but it shouldn't be understood as the only scope. In fact, the two most prominent study designs in population genetics are perfect illustrations of the computational complexity associated with Targeted Learning.","category":"page"},{"location":"tmle_estimation/#Preliminary","page":"Targeted Minimum Loss Based Estimation","title":"Preliminary","text":"","category":"section"},{"location":"tmle_estimation/","page":"Targeted Minimum Loss Based Estimation","title":"Targeted Minimum Loss Based Estimation","text":"Remember that for each estimand of interest, Targeted Learning requires 3 main ingredients that drive computational complexity:","category":"page"},{"location":"tmle_estimation/","page":"Targeted Minimum Loss Based Estimation","title":"Targeted Minimum Loss Based Estimation","text":"An estimator for the propensity score: G(T, W) = P(T|W).\nAn estimator for the outcome's mean: Q(T, W) = E[Y|T, W].\nA targeting step towards the estimand of interest.","category":"page"},{"location":"tmle_estimation/","page":"Targeted Minimum Loss Based Estimation","title":"Targeted Minimum Loss Based Estimation","text":"While the targeting step has a fixed form, Both G and Q require specification of learning algorithms that can range from simple generalized linear models to complex Super Learners. In general, one doesn't know how the data has been generated and the model space should be kept as large as possible in order to provide valid inference. This means we recommend to use Super Learning for both G and Q as it comes with asymptotic theoretical guarantees. However, Super Learning is an expensive procedure and, depending on the context, might become infeasible. Also, notice that while the targeting step is specific to a given estimand, G and Q are only specific to the variables occuring in the causal graph. This means that they can potentially be cleverly reused across the estimation of multiple estimands. Note that this clever reuse, is already baked into this package, and nothing needs to be done beside specifying the learning algorithms for G and Q. The goal of the subsequent sections is to provide some examples, guiding the choice of those learning algorithms.","category":"page"},{"location":"tmle_estimation/","page":"Targeted Minimum Loss Based Estimation","title":"Targeted Minimum Loss Based Estimation","text":"In what follows, Y is an outcome of interest, W a set of confounding variables and T a genetic variation. Genetic variations are usually represented as a pair of alleles corresponding to an individual's genotype. We will further restrict the scope to bi-allelic single nucleotide variations. This means that, at a given locus where the two alleles are A and C, an individual could have any of the following genotype: AA, AC, CC.","category":"page"},{"location":"tmle_estimation/","page":"Targeted Minimum Loss Based Estimation","title":"Targeted Minimum Loss Based Estimation","text":"The various estimators used below are further described in Ready to use estimator files","category":"page"},{"location":"tmle_estimation/#Multiple-treatment-contrasts","page":"Targeted Minimum Loss Based Estimation","title":"Multiple treatment contrasts","text":"","category":"section"},{"location":"tmle_estimation/","page":"Targeted Minimum Loss Based Estimation","title":"Targeted Minimum Loss Based Estimation","text":"In a classic randomized control trial, the treatment variable can only take one of two levels: treated or not treated. In out example however, any genetic variation takes its values from three different levels. For instance, one could be any of AA, AC or CC at a given locus. As such, the treated and not treated levels need to be defined and any of the following contrasts can be of interest:","category":"page"},{"location":"tmle_estimation/","page":"Targeted Minimum Loss Based Estimation","title":"Targeted Minimum Loss Based Estimation","text":"AA -> AC\nAC -> CC\nAA -> CC","category":"page"},{"location":"tmle_estimation/","page":"Targeted Minimum Loss Based Estimation","title":"Targeted Minimum Loss Based Estimation","text":"For a given outcome and genetic variation, for each contrast, both G and Q are actually the same. This shows a first level of reduction in computational complexity. Both G and Q need to be fitted only once across multiple treatment contrasts and only the targeting step needs to be carried out again.","category":"page"},{"location":"tmle_estimation/#The-PheWAS-study-design","page":"Targeted Minimum Loss Based Estimation","title":"The PheWAS study design","text":"","category":"section"},{"location":"tmle_estimation/","page":"Targeted Minimum Loss Based Estimation","title":"Targeted Minimum Loss Based Estimation","text":"In a PheWAS, one is interested in the effect of a genetic variation across many outcomes (typically around 1000). Because the treatment variable is always the same, the propensity score G can be reused across all parameters, which drastically reduces computational complexity.","category":"page"},{"location":"tmle_estimation/","page":"Targeted Minimum Loss Based Estimation","title":"Targeted Minimum Loss Based Estimation","text":"<div style=\"text-align:center\">\n<img src=\"assets/phewas.png\" alt=\"PheWAS\" style=\"width:400px;\"/>\n</div>","category":"page"},{"location":"tmle_estimation/","page":"Targeted Minimum Loss Based Estimation","title":"Targeted Minimum Loss Based Estimation","text":"With this setup in mind, the complexity is mostly driven by the specification of the learning algorithms for Q, which will have to be fitted for each outcome. For each outcome, we estimate the 3 Average Treatment Effects corresponding to the 3 possible treatment contrasts defined in the previous section. There are thus two levels of reuse of G and Q in this study design. In the table below are presented some runtimes for various specifications of G and Q using a single cpu. The \"Unit runtime\" is the average runtime across all estimands and can roughly be extrapolated to bigger studies.","category":"page"},{"location":"tmle_estimation/","page":"Targeted Minimum Loss Based Estimation","title":"Targeted Minimum Loss Based Estimation","text":"Estimator file Unit runtime (s) Extrapolated runtime to 1000 outcomes\ndocs/src/estimators/glm.jl 4.65 1h20\ndocs/src/estimators/glmnet.jl 7.31 2h\ndocs/src/estimators/G-superlearning-Q-glmnet.jl 49.87 13h45\ndocs/src/estimators/superlearning.jl ? ?","category":"page"},{"location":"tmle_estimation/","page":"Targeted Minimum Loss Based Estimation","title":"Targeted Minimum Loss Based Estimation","text":"Depending on the exact setup, this means one can probably afford to use Super Learning for at least the estimation of G (and potentially also for Q for a single PheWAS). This turns out to be a great news because TMLE is a double robust estimator. As a reminder, it means that only one of the estimators for G or Q needs to converge sufficiently fast to the ground truth to guarantee that our estimates will be asymptotically unbiased.","category":"page"},{"location":"tmle_estimation/","page":"Targeted Minimum Loss Based Estimation","title":"Targeted Minimum Loss Based Estimation","text":"Finally, note that those runtime estimates should be interpreted as worse cases, this is because:","category":"page"},{"location":"tmle_estimation/","page":"Targeted Minimum Loss Based Estimation","title":"Targeted Minimum Loss Based Estimation","text":"Only 1 cpu is used.\nMost modern high performance computing platform will allow further parallelization.\nIn the case where G only is a Super Learner, since the number of parameters is still relatively low, it is possible that the time to fit G still dominates the runtime.\nRuntimes include precompilation which becomes negligible with the size of the study.","category":"page"},{"location":"tmle_estimation/#The-GWAS-study-design","page":"Targeted Minimum Loss Based Estimation","title":"The GWAS study design","text":"","category":"section"},{"location":"tmle_estimation/","page":"Targeted Minimum Loss Based Estimation","title":"Targeted Minimum Loss Based Estimation","text":"In a GWAS, the outcome variable is held fixed and we are interested in the effects of very many genetic variations on this outcome (typically 800 000 for a genotyping array). The propensity score cannot be reused across parameters resulting in a more expensive run.","category":"page"},{"location":"tmle_estimation/","page":"Targeted Minimum Loss Based Estimation","title":"Targeted Minimum Loss Based Estimation","text":"<div style=\"text-align:center\">\n<img src=\"assets/gwas.png\" alt=\"GWAS\" style=\"width:400px;\"/>\n</div>","category":"page"},{"location":"tmle_estimation/","page":"Targeted Minimum Loss Based Estimation","title":"Targeted Minimum Loss Based Estimation","text":"Again, we estimate the 3 Average Treatment Effects corresponding to the 3 possible treatment contrasts for each genetic variation. In the table below are presented some runtimes for various specifications of G and Q using a single cpu. The \"Unit runtime\" is the average runtime across all estimands and can roughly be extrapolated to bigger studies.","category":"page"},{"location":"tmle_estimation/","page":"Targeted Minimum Loss Based Estimation","title":"Targeted Minimum Loss Based Estimation","text":"Estimator file Continuous outcome unit runtime (s) Binary outcome unit runtime (s)\ndocs/src/estimators/glm.jl 5.86 6.42\ndocs/src/estimators/glmnet.jl 17.55164408683777 22.34\ndocs/src/estimators/G-superlearning-Q-glmnet.jl 435.95 ?\ndocs/src/estimators/superlearning.jl ? ?","category":"page"},{"location":"tmle_estimation/","page":"Targeted Minimum Loss Based Estimation","title":"Targeted Minimum Loss Based Estimation","text":"It is thus quite unlikely that you will be able to use Super Learning for any of P(V|W) or E[Y|V, W] if you don't have access to a high performance computing platform. For practical purposes it might be necessary to use a GLM or GLMNet. As such, no double robustness guarantee will be satisfied in general. However, our estimate will still be targeted, which means that its bias will be reduced compared to classic inference using a parametric model.","category":"page"}]
}
